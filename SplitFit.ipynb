{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2285ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Sampler, BatchSampler, Dataset, DataLoader, Subset, SubsetRandomSampler, random_split\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "import lmfit\n",
    "from scipy import interpolate\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0eb7c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genData(iNS,iNB):\n",
    "    sx=np.random.normal(0,0.5,iNS)\n",
    "    sy=np.random.uniform(0.1,1,iNS)\n",
    "    sz=np.random.triangular(0.,0.95, 1, iNS)\n",
    "    s=np.vstack([sx,sy,sz])\n",
    "    \n",
    "    bx=np.random.uniform(-3,3,iNB)\n",
    "    by=np.random.uniform(-1,-0.1,iNB)\n",
    "    bz=np.random.triangular(0,0.05,1,iNB)\n",
    "    b=np.vstack([bx,by,bz])\n",
    "    return s,b\n",
    "\n",
    "sig,bkg=genData(100,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3f584ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        super(DataSet, self).__init__()\n",
    "        self.labels  = labels\n",
    "        self.samples = samples\n",
    "        if len(samples) != len(labels):\n",
    "            raise ValueError(\n",
    "                f\"should have the same number of samples({len(samples)}) as there are labels({len(labels)})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        y = self.labels[index]\n",
    "        x = self.samples[index]\n",
    "        return x, y\n",
    "\n",
    "class simple_MLPFit_onelayer(torch.nn.Module):\n",
    "    def __init__(self,in_data,input_size,out_channels=1,act_out=False,nhidden=16,batchnorm=False,batch_size=500,n_epochs=100):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, nhidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nhidden, nhidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nhidden, out_channels),\n",
    "        )\n",
    "        self.loss    = sigLoss()\n",
    "        self.output  = torch.nn.Sigmoid()\n",
    "        self.act_out = act_out\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs     = n_epochs\n",
    "        self.opt     = torch.optim.Adam(self.model.parameters(),lr=0.0002)\n",
    "        self.dataloader = DataLoader(in_data, batch_size=self.batch_size, shuffle=True)#,pin_memory=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)        \n",
    "        if self.act_out:\n",
    "            x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "    def training_mse_epoch(self):\n",
    "        running_loss = 0.0\n",
    "        updates=0\n",
    "        for batch_idx, (x, y) in enumerate(self.dataloader):\n",
    "            self.opt.zero_grad()\n",
    "            #x     = x.cuda(); y = y.cuda()\n",
    "            #x = x.reshape((self.batch_size,1))\n",
    "            x = x.reshape((len(x),1))\n",
    "            x_out = self.forward(x)\n",
    "            loss  = self.loss(x_out.flatten(), y.flatten())\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            running_loss += loss \n",
    "            updates = updates+1\n",
    "        return running_loss/updates\n",
    "\n",
    "    def training_mse(self):\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.model.train(True)\n",
    "            loss_train = self.training_mse_epoch()\n",
    "\n",
    "            #self.model2.train(False)\n",
    "            #loss_valid = self.validate_mse_epoch(self.model2,self.val_dataloader_mse())\n",
    "            if epoch % 10 == 0:\n",
    "                print('Epoch: {} LOSS train: {} '.format(epoch,loss_train))\n",
    "\n",
    "\n",
    "def makeDataSet(iD,iNS,iNB):\n",
    "    #1 is going to be sideband\n",
    "    #2 is going to be mass range\n",
    "    pD =  torch.from_numpy(iD.T)\n",
    "    pXD =  pD[torch.randperm(len(pD))]\n",
    "    tot=pXD[:,2]\n",
    "    tot=tot.float()\n",
    "    label=pXD[:,0]\n",
    "    label=label.float()\n",
    "    datatrain=DataSet(samples=tot,labels=label)\n",
    "    return datatrain,pD[0:iNS].float(),pD[iNS-1:-1].float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "602be4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0180)\n"
     ]
    }
   ],
   "source": [
    "#import torchsort\n",
    "\n",
    "def differentiable_histogram(x, weights, bins=3, min=-3.0, max=3.0):\n",
    "\n",
    "    #if len(x.shape) == 4:\n",
    "    #    n_samples, n_chns, _, _ = x.shape\n",
    "    #elif len(x.shape) == 2:\n",
    "    #    n_samples, n_chns = 1, 1\n",
    "    #else:\n",
    "    #    raise AssertionError('The dimension of input tensor should be 2 or 4.')\n",
    "    n_samples, n_chns = 1, 1\n",
    "    hist_torch = torch.zeros(n_samples, n_chns, bins).to(x.device)\n",
    "    delta = (max - min) / bins\n",
    "\n",
    "    BIN_Table = torch.range(start=0, end=bins, step=1) * delta\n",
    "\n",
    "    for dim in range(1, bins-1, 1):\n",
    "        h_r = BIN_Table[dim].item()             # h_r\n",
    "        h_r_sub_1 = BIN_Table[dim - 1].item()   # h_(r-1)\n",
    "        h_r_plus_1 = BIN_Table[dim + 1].item()  # h_(r+1)\n",
    "\n",
    "        mask_sub = ((h_r > x) & (x >= h_r_sub_1)).float()\n",
    "        mask_plus = ((h_r_plus_1 > x) & (x >= h_r)).float()\n",
    "        print(\"mask\",mask_sub.shape,\"hr\",h_r,\"hrsub\",h_r_sub_1)\n",
    "        print(\"val 0\", (x - h_r_sub_1)* mask_sub)\n",
    "        print(\"val 1\", (h_r_plus_1 - x)*mask_plus)\n",
    "        hist_torch[:, :, dim] += torch.sum(((x - h_r_sub_1) * mask_sub).view(n_samples, n_chns, -1), dim=-1)\n",
    "        hist_torch[:, :, dim] += torch.sum(((h_r_plus_1 - x) * mask_plus).view(n_samples, n_chns, -1), dim=-1)\n",
    "        print(\"dim\",dim)\n",
    "        print(\"hist torch\",hist_torch.shape)\n",
    "        print(\"diff:\",x - h_r_sub_1)\n",
    "\n",
    "    return hist_torch / delta\n",
    "\n",
    "class sigLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, sort_tolerance=1.0,sort_reg='l2'):\n",
    "        super(sigLoss, self).__init__()\n",
    "        self.tolerance = sort_tolerance\n",
    "        self.reg       = sort_reg\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        #loss = (1.0 - (torch.tanh (alpha * (x[1:] - x[:-1])) * torch.tanh (alpha * (y[1:] - y[:1])))).mean()\n",
    "        #xsort=torchsort.soft_rank(x.reshape(1,-1))\n",
    "        #xsort=torch.sort(x.reshape(1,-1))\n",
    "        #xsort=torch.topk(x.reshape(1,-1), 25, dim=-1)\n",
    "        #cut=xsort.values\n",
    "        weight=0.5*(torch.tanh (5.0*(x-0.5))+1.0)\n",
    "        #print(\"!\",x,\"W\",weight)\n",
    "        #sig = s/sqrt(s+b) \n",
    "        #yhist, bin_edges   = torch.histogram(y, bins=3,range=[-3,3],weight=weight,density=True)\n",
    "        #ynorm   = torch.sum(weight*(y > -1)*(y < 1))\n",
    "        #yvalues = (weight*(y > -1)*(y < 1)-0.5*weight*(y < -1)-0.5*weight*(y > 1))#\n",
    "        #loss=10.0-torch.sum(yvalues)/torch.sqrt(ynorm)\n",
    "        yhist = differentiable_histogram(y,weight)\n",
    "        print(yhist)\n",
    "        loss=yhist[1]-0.5*(yhist[0]+yhist[2])\n",
    "        return loss\n",
    "\n",
    "    def significance(self,iX,iY):\n",
    "        print(iX,iY)\n",
    "        NLL = deltaNLL(iX,iY,False)\n",
    "        return NLL\n",
    "\n",
    "val=torch.tensor(0.3)\n",
    "print(0.5*(torch.tanh(10*(val-0.5))+1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4aec41dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask torch.Size([500]) hr 2.0 hrsub 0.0\n",
      "val 0 tensor([0.8441, 0.0872, -0.0000, 1.4599, -0.0000, -0.0000, 0.0000, -0.0000, 0.9452,\n",
      "        -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 1.8578, -0.0000, -0.0000, 0.2312,\n",
      "        -0.0000, 1.5823, -0.0000, 0.0000, -0.0000, 1.7369, 0.0998, -0.0000, -0.0000,\n",
      "        0.0000, 1.4957, 1.3157, -0.0000, -0.0000, 1.9034, -0.0000, -0.0000, 0.9324,\n",
      "        -0.0000, 0.0000, -0.0000, -0.0000, 0.0877, 0.0000, 1.8359, 0.3853, 0.0000,\n",
      "        -0.0000, 0.0000, 0.0000, 1.3902, 0.9857, 0.5066, -0.0000, -0.0000, -0.0000,\n",
      "        0.0000, 0.0000, 0.0456, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        1.4303, -0.0000, -0.0000, 1.0355, -0.0000, 0.4813, 1.1978, 0.0000, 0.7552,\n",
      "        1.6093, -0.0000, 0.0000, 0.0000, 0.1743, -0.0000, 0.0000, -0.0000, 0.0000,\n",
      "        -0.0000, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        0.1557, 1.4426, 0.8321, 1.4240, 0.0000, -0.0000, 0.3013, 1.1230, 1.8218,\n",
      "        -0.0000, 1.3551, -0.0000, -0.0000, 1.5209, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, 1.7420, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000, 1.6710, -0.0000,\n",
      "        0.0000, 0.4669, -0.0000, -0.0000, -0.0000, 1.9579, -0.0000, 0.0000, -0.0000,\n",
      "        1.6645, -0.0000, 0.0000, -0.0000, 0.0000, 0.0251, 1.0209, -0.0000, -0.0000,\n",
      "        -0.0000, 0.3345, -0.0000, 1.4190, -0.0000, 0.5564, -0.0000, -0.0000, -0.0000,\n",
      "        0.1592, 1.1306, -0.0000, 1.6984, -0.0000, 1.5127, 0.9992, -0.0000, 0.0000,\n",
      "        -0.0000, -0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 1.8245, 0.4224, -0.0000,\n",
      "        1.3385, 0.5220, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 1.1601,\n",
      "        -0.0000, -0.0000, -0.0000, 1.4900, 0.6454, -0.0000, -0.0000, -0.0000, 0.0000,\n",
      "        -0.0000, -0.0000, 0.2716, 0.1821, 0.7653, -0.0000, -0.0000, 0.9369, 0.6517,\n",
      "        -0.0000, -0.0000, 0.0000, 0.9751, -0.0000, -0.0000, 1.2378, -0.0000, 1.8431,\n",
      "        1.0050, -0.0000, 0.0000, -0.0000, -0.0000, 1.1420, -0.0000, 0.0000, 0.0000,\n",
      "        -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.4987, 0.3483, -0.0000,\n",
      "        0.0000, -0.0000, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, 0.5155, -0.0000, -0.0000, -0.0000, -0.0000, 1.4817, 1.1479,\n",
      "        -0.0000, -0.0000, -0.0000, 0.5220, 0.0000, 0.8656, -0.0000, 1.1035, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, 0.8763, -0.0000, 1.3881, 0.0000, 0.0000, -0.0000, -0.0000, 1.0370,\n",
      "        1.7987, -0.0000, 0.0000, 0.3119, -0.0000, 0.0000, 0.0000, -0.0000, -0.0000,\n",
      "        0.3975, -0.0000, -0.0000, 0.0000, 1.5143, -0.0000, 0.6036, 1.1224, -0.0000,\n",
      "        1.6829, 0.0299, 1.2021, 0.5065, 0.1180, -0.0000, -0.0000, 0.1616, 0.5134,\n",
      "        1.7038, 0.4623, -0.0000, 0.8039, -0.0000, 0.7444, -0.0000, -0.0000, 1.0856,\n",
      "        -0.0000, -0.0000, -0.0000, 0.2115, -0.0000, 1.8533, 1.1162, 0.2434, 0.0000,\n",
      "        -0.0000, 0.4623, -0.0000, -0.0000, 0.3813, 0.8329, -0.0000, 1.2483, 0.4896,\n",
      "        0.9471, 0.2671, 0.0000, -0.0000, 0.0000, -0.0000, -0.0000, 1.0210, 0.1709,\n",
      "        0.0000, 0.4202, -0.0000, 1.3724, -0.0000, 0.4031, -0.0000, -0.0000, 1.9003,\n",
      "        1.5698, 0.5710, 0.5297, 0.0000, 1.9703, -0.0000, 1.2051, -0.0000, 0.0000,\n",
      "        -0.0000, -0.0000, 1.4914, 1.9254, 0.0000, 0.0000, -0.0000, 0.2098, -0.0000,\n",
      "        0.0000, 0.3332, 0.0000, 1.3446, -0.0000, -0.0000, -0.0000, 1.2755, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, 0.4632, -0.0000, -0.0000, 0.0000, 0.0000,\n",
      "        -0.0000, 0.6895, 0.4880, 0.0000, 0.0000, 0.0000, 0.7728, 0.0000, 0.0000,\n",
      "        -0.0000, -0.0000, 1.1786, -0.0000, -0.0000, -0.0000, 0.0000, 1.2773, 0.0000,\n",
      "        -0.0000, 0.8522, 0.3916, -0.0000, 0.0288, -0.0000, -0.0000, 1.9520, -0.0000,\n",
      "        -0.0000, 1.3419, -0.0000, 0.2205, 1.0785, 1.8093, 1.6490, 0.0000, 0.4277,\n",
      "        1.3095, -0.0000, 1.1214, 0.9621, 0.6898, -0.0000, 0.0000, 0.0000, -0.0000,\n",
      "        0.0000, 1.9495, -0.0000, -0.0000, 1.0247, -0.0000, -0.0000, 0.0000, 0.2216,\n",
      "        -0.0000, 1.7384, 0.3140, -0.0000, -0.0000, -0.0000, 1.3534, 1.2282, 0.0000,\n",
      "        0.0000, -0.0000, 0.0000, 0.6052, 0.0000, 0.0000, 0.9465, 0.0000, 0.0000,\n",
      "        -0.0000, 0.0000, 0.4672, -0.0000, -0.0000, -0.0000, -0.0000, 0.3367, 0.0000,\n",
      "        0.0000, 1.8095, 1.6317, -0.0000, -0.0000, -0.0000, -0.0000, 0.0000, 0.1891,\n",
      "        -0.0000, -0.0000, -0.0000, 0.9705, -0.0000, -0.0000, 0.0000, -0.0000, -0.0000,\n",
      "        0.0000, 1.8634, -0.0000, -0.0000, -0.0000, 0.0000, 0.8687, 1.9233, 1.6271,\n",
      "        1.3497, -0.0000, 0.0000, -0.0000, 1.3203, 1.7819, -0.0000, 0.0000, 1.9761,\n",
      "        0.3295, -0.0000, -0.0000, -0.0000, 1.4336])\n",
      "val 1 tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.4149, 0.0000, 0.0000,\n",
      "        0.0000, 1.0517, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.5079, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.5494, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 1.4688, 0.0000, 0.0000, 0.0000, 1.5099, 0.0000, 0.0000, 1.3599,\n",
      "        0.0000, 1.9413, 1.3976, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.7992, 1.4626, 0.0000, 1.7058, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8911, 0.0000,\n",
      "        0.0000, 0.0000, 1.8949, 1.7477, 0.0000, 0.0000, 1.4287, 0.0000, 1.2507,\n",
      "        0.0000, 0.0000, 1.0671, 1.2779, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.8706, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 1.5597, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.8100, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.4668, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7240, 0.0000,\n",
      "        0.0000, 0.0000, 1.2547, 0.0000, 1.1489, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3712,\n",
      "        0.0000, 0.0000, 1.4385, 0.0000, 1.1272, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3946,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.4452, 0.0000, 0.0000, 0.0000, 0.0000, 1.3688, 1.1982,\n",
      "        0.0000, 1.7684, 0.0000, 0.0000, 0.0000, 1.8829, 0.0000, 0.0000, 0.0000,\n",
      "        1.9110, 0.0000, 0.0000, 0.0000, 1.8211, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.1854, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5780, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.2968, 1.8898, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.8934, 0.0000, 0.0000, 1.6623, 1.5721, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.1439, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5549,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.9685, 0.0000, 1.0154, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.7692, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 1.5194, 0.0000, 0.0000, 0.0000, 0.0000, 1.5591,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0346, 1.6094, 0.0000, 0.0000, 0.0000,\n",
      "        1.1798, 0.0000, 1.9410, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3554, 1.0463,\n",
      "        0.0000, 0.0000, 0.0000, 1.9859, 1.6150, 1.0035, 0.0000, 1.9916, 1.0984,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.5431, 0.0000, 1.5135,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0238, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.8610, 1.5722, 0.0000,\n",
      "        1.0986, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.2999, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0368,\n",
      "        1.6306, 0.0000, 1.1652, 0.0000, 1.6422, 1.0463, 0.0000, 1.6637, 1.5547,\n",
      "        0.0000, 1.7946, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1915,\n",
      "        1.6041, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0223, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1832, 0.0000, 0.0000,\n",
      "        1.9185, 0.0000, 0.0000, 0.0000, 0.0000, 1.6608, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 1.6815, 0.0000, 0.0000, 0.0000, 0.0000, 1.5484, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n",
      "dim 1\n",
      "hist torch torch.Size([1, 1, 3])\n",
      "diff: tensor([ 0.8441,  0.0872, -0.7168,  1.4599, -2.0099, -2.7906,  2.5851, -1.4012,\n",
      "         0.9452, -2.8442,  2.9483, -1.2143, -2.9023, -0.6964,  1.8578, -0.2798,\n",
      "        -2.0968,  0.2312, -1.8616,  1.5823, -2.7601,  2.4921, -1.8596,  1.7369,\n",
      "         0.0998, -1.8277, -2.3739,  2.4506,  1.4957,  1.3157, -1.5177, -1.4713,\n",
      "         1.9034, -1.1748, -1.9401,  0.9324, -0.1378,  2.5312, -2.9405, -1.4768,\n",
      "         0.0877,  2.4901,  1.8359,  0.3853,  2.6401, -1.8718,  2.0587,  2.6024,\n",
      "         1.3902,  0.9857,  0.5066, -2.2126, -1.7981, -2.3597,  2.2008,  2.5374,\n",
      "         0.0456,  2.2942, -0.2765, -2.9616, -2.9218, -2.0106, -0.3669,  1.4303,\n",
      "        -1.5554, -2.4138,  1.0355, -1.2003,  0.4813,  1.1978,  2.1089,  0.7552,\n",
      "         1.6093, -2.4317,  2.1051,  2.2523,  0.1743, -2.8631,  2.5713, -0.1470,\n",
      "         2.7493, -1.8615, -1.8259,  2.9329,  2.7221, -1.8131, -0.8196, -0.1413,\n",
      "        -0.2267, -0.5863,  0.1557,  1.4426,  0.8321,  1.4240,  2.1294, -0.0445,\n",
      "         0.3013,  1.1230,  1.8218, -0.4892,  1.3551, -2.6077, -1.4067,  1.5209,\n",
      "        -0.9857, -1.9552, -0.6017, -0.3568, -2.6299,  2.4403, -2.4531, -0.1295,\n",
      "        -0.7905, -2.2217, -0.5132, -2.5606, -1.3325, -2.2017,  1.7420, -0.5049,\n",
      "        -0.7087,  2.1900, -2.7753, -0.5312,  1.6710, -2.0184,  2.5332,  0.4669,\n",
      "        -0.2957, -0.9117, -0.7558,  1.9579, -0.1360,  2.2760, -0.7148,  1.6645,\n",
      "        -2.6876,  2.7453, -2.1202,  2.8511,  0.0251,  1.0209, -1.1568, -0.1280,\n",
      "        -2.7743,  0.3345, -1.9489,  1.4190, -2.5565,  0.5564, -0.0743, -0.4863,\n",
      "        -0.1780,  0.1592,  1.1306, -0.1985,  1.6984, -0.7046,  1.5127,  0.9992,\n",
      "        -1.1416,  2.6288, -0.6758, -0.6733,  2.5615, -1.3429,  2.8728, -2.4452,\n",
      "         1.8245,  0.4224, -1.4845,  1.3385,  0.5220, -1.4599, -1.8102, -1.8144,\n",
      "        -2.7437, -1.2503, -2.7729,  1.1601, -2.2123, -0.8462, -1.4184,  1.4900,\n",
      "         0.6454, -0.3885, -1.4314, -2.5121,  2.6054, -0.9472, -2.6010,  0.2716,\n",
      "         0.1821,  0.7653, -1.7792, -0.8541,  0.9369,  0.6517, -0.1652, -0.8097,\n",
      "         2.0549,  0.9751, -0.8518, -2.7781,  1.2378, -0.3448,  1.8431,  1.0050,\n",
      "        -2.8854,  2.5548, -2.3086, -0.4676,  1.1420, -2.0972,  2.6312,  2.8018,\n",
      "        -2.0885,  2.2316, -1.0000, -0.6297, -2.9852,  2.1171,  0.4987,  0.3483,\n",
      "        -2.1419,  2.0890, -1.1682, -0.2014, -1.6440,  2.1789, -2.5388, -1.6477,\n",
      "        -1.0349, -1.1910, -0.6571, -2.3006,  0.5155, -0.5033, -2.7015, -0.3793,\n",
      "        -0.4200,  1.4817,  1.1479, -0.9613, -0.1686, -2.0377,  0.5220,  2.8146,\n",
      "         0.8656, -0.1564,  1.1035, -1.3452, -2.0083, -2.2497, -2.9750, -2.4029,\n",
      "        -1.1536, -1.8648,  2.4220, -2.0895, -0.5581, -2.0057,  0.8763, -2.6714,\n",
      "         1.3881,  2.7032,  2.1102, -2.7463, -2.7727,  1.0370,  1.7987, -2.1718,\n",
      "         2.1066,  0.3119, -1.2957,  2.3377,  2.4279, -0.3436, -2.4701,  0.3975,\n",
      "        -0.9309, -2.0342,  2.8561,  1.5143, -2.4327,  0.6036,  1.1224, -2.6273,\n",
      "         1.6829,  0.0299,  1.2021,  0.5065,  0.1180, -2.7405, -1.7665,  0.1616,\n",
      "         0.5134,  1.7038,  0.4623, -2.7202,  0.8039, -1.7101,  0.7444, -1.4624,\n",
      "        -0.6893,  1.0856, -2.5578, -1.2215, -1.7282,  0.2115, -1.5270,  1.8533,\n",
      "         1.1162,  0.2434,  2.4451, -1.8995,  0.4623, -1.4744, -1.9308,  0.3813,\n",
      "         0.8329, -1.1141,  1.2483,  0.4896,  0.9471,  0.2671,  2.0315, -2.0532,\n",
      "         2.9846, -1.6630, -0.3445,  1.0210,  0.1709,  2.2308,  0.4202, -0.4279,\n",
      "         1.3724, -0.3094,  0.4031, -0.3200, -2.8771,  1.9003,  1.5698,  0.5710,\n",
      "         0.5297,  2.4806,  1.9703, -2.8816,  1.2051, -0.8719,  2.4409, -1.3667,\n",
      "        -2.0580,  1.4914,  1.9254,  2.9654,  2.3906, -1.3667,  0.2098, -2.1555,\n",
      "         2.8202,  0.3332,  2.0590,  1.3446, -1.7158, -2.3593, -1.0205,  1.2755,\n",
      "        -0.5510, -2.4518, -0.8645, -1.5417, -2.9674,  0.4632, -2.0473, -2.9886,\n",
      "         2.6446,  2.9537, -1.7713,  0.6895,  0.4880,  2.0141,  2.3850,  2.9965,\n",
      "         0.7728,  2.0084,  2.9016, -0.2326, -1.9660,  1.1786, -1.6592, -2.5459,\n",
      "        -1.9071,  2.4569,  1.2773,  2.4865, -1.5348,  0.8522,  0.3916, -1.1244,\n",
      "         0.0288, -0.9797, -2.8730,  1.9520, -1.8822, -0.7560,  1.3419, -1.9443,\n",
      "         0.2205,  1.0785,  1.8093,  1.6490,  2.9762,  0.4277,  1.3095, -1.9847,\n",
      "         1.1214,  0.9621,  0.6898, -2.6539,  2.1390,  2.4278, -1.8578,  2.9014,\n",
      "         1.9495, -0.9243, -1.5670,  1.0247, -2.2348, -1.6868,  2.7001,  0.2216,\n",
      "        -1.9197,  1.7384,  0.3140, -2.5344, -2.4643, -0.5110,  1.3534,  1.2282,\n",
      "         2.9632,  2.3694, -1.2530,  2.8348,  0.6052,  2.3578,  2.9537,  0.9465,\n",
      "         2.3363,  2.4453, -2.2095,  2.2054,  0.4672, -2.7316, -0.0722, -1.2841,\n",
      "        -2.6664,  0.3367,  2.8085,  2.3959,  1.8095,  1.6317, -1.6031, -0.7007,\n",
      "        -0.9490, -1.8285,  2.9777,  0.1891, -2.0504, -2.7267, -2.6742,  0.9705,\n",
      "        -1.4863, -0.2755,  2.8168, -2.5877, -0.2208,  2.0815,  1.8634, -0.8308,\n",
      "        -2.8333, -2.9323,  2.3392,  0.8687,  1.9233,  1.6271,  1.3497, -0.1683,\n",
      "         2.3185, -1.8571,  1.3203,  1.7819, -2.2234,  2.4516,  1.9761,  0.3295,\n",
      "        -0.8071, -2.2569, -2.1708,  1.4336])\n",
      "tensor([[[  0.0000, 148.1991,   0.0000]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4t/mwl83f3x1ls0jvy35v4qzj7c0000gp/T/ipykernel_87277/3901227235.py:15: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  BIN_Table = torch.range(start=0, end=bins, step=1) * delta\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[212], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m data,test_sig,test_bkg\u001b[38;5;241m=\u001b[39mmakeDataSet(pdata,\u001b[38;5;28mlen\u001b[39m(sig[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mlen\u001b[39m(bkg[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     30\u001b[0m rw_model \u001b[38;5;241m=\u001b[39m simple_MLPFit_onelayer(data,\u001b[38;5;241m1\u001b[39m,out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,act_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,batchnorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mrw_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m output_sig\u001b[38;5;241m=\u001b[39mrw_model\u001b[38;5;241m.\u001b[39mforward(test_sig[:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(test_sig),\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     33\u001b[0m output_bkg\u001b[38;5;241m=\u001b[39mrw_model\u001b[38;5;241m.\u001b[39mforward(test_bkg[:,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(test_bkg),\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[180], line 61\u001b[0m, in \u001b[0;36msimple_MLPFit_onelayer.training_mse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 61\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_mse_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m#self.model2.train(False)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m#loss_valid = self.validate_mse_epoch(self.model2,self.val_dataloader_mse())\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[180], line 51\u001b[0m, in \u001b[0;36msimple_MLPFit_onelayer.training_mse_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(x),\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     50\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m---> 51\u001b[0m loss  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[211], line 57\u001b[0m, in \u001b[0;36msigLoss.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     55\u001b[0m yhist \u001b[38;5;241m=\u001b[39m differentiable_histogram(y,weight)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(yhist)\n\u001b[0;32m---> 57\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[43myhist\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\u001b[38;5;241m*\u001b[39m(yhist[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39myhist[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "def cutval(iData, p=0.9):\n",
    "    datasort=np.sort(iData)\n",
    "    ndata=len(datasort)\n",
    "    return datasort[int(ndata*p)]\n",
    "\n",
    "def plotHistComp(iGSig,iGBkg,iSig,iBkg,iMin=-3,iMax=3):\n",
    "    ns=len(iSig)\n",
    "    nb=len(iBkg)\n",
    "    ngs=len(iGSig)\n",
    "    ngb=len(iGBkg)\n",
    "    print(ns,nb,ngs,ngb)\n",
    "    ys, bin_edges = np.histogram(iSig,density=True,bins=20,range=[iMin,iMax])#bins=bin_edges\n",
    "    yb, bin_edges = np.histogram(iBkg, bins=bin_edges,density=True)\n",
    "    ygs,bin_edges = np.histogram(iGSig, bins=bin_edges,density=True)\n",
    "    ygb,bin_edges = np.histogram(iGBkg, bins=bin_edges,density=True)\n",
    "    ygs*=len(iGSig)/len(iSig)\n",
    "    ygb*=len(iGBkg)/len(iBkg)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    plt.errorbar(bin_centers,yb,yerr=(nb*yb)**0.5/nb,marker='.',linestyle = '-', color = 'red',label='bkg')\n",
    "    plt.errorbar(bin_centers,ys,yerr=(ns*ys)**0.5/ns,marker='.',linestyle = '-', color = 'blue',label='signal')\n",
    "    plt.errorbar(bin_centers,ygb,yerr=(ngb/nb)*((ngb*ygb)**0.5)/ngb,marker='.',linestyle = '--', color = 'red',label='bkg(no cuts)')\n",
    "    plt.errorbar(bin_centers,ygs,yerr=(ngs/ns)*((ngs*ygs)**0.5)/ngs,marker='.',linestyle = '--', color = 'blue',label='signal(no cuts)')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Normalized\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "pdata = np.hstack((sig,bkg))\n",
    "data,test_sig,test_bkg=makeDataSet(pdata,len(sig[0]),len(bkg[0]))\n",
    "rw_model = simple_MLPFit_onelayer(data,1,out_channels=1,act_out=True,batchnorm=False)\n",
    "rw_model.training_mse()\n",
    "output_sig=rw_model.forward(test_sig[:,2].reshape(len(test_sig),1))\n",
    "output_bkg=rw_model.forward(test_bkg[:,2].reshape(len(test_bkg),1))\n",
    "output=torch.cat((output_sig,output_bkg))\n",
    "cut=cutval(output.flatten().detach().numpy(),p=0.9)\n",
    "_,bins,_=plt.hist(output_sig.flatten().detach().numpy(),density=True,alpha=0.5,label='sig')\n",
    "plt.hist(output_bkg.flatten().detach().numpy(),density=True,alpha=0.5,label='bkg',bins=bins)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "cutsig = output_sig.flatten().detach().numpy() > cut\n",
    "cutbkg = output_bkg.flatten().detach().numpy() > cut\n",
    "plotHistComp(test_sig[:,0],test_bkg[:,0],test_sig[cutsig][:,0],test_bkg[cutbkg][:,0])\n",
    "plotHistComp(test_sig[:,2],test_bkg[:,2],test_sig[cutsig][:,2],test_bkg[cutbkg][:,2],0,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982de3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
